# 🗓️ 2025년 10월 3일 TIL

## 📊 Today's Summary
- 5개 수어 단어에 대해 각 4개씩 추가 영상 촬영 (총 20개)
- 로컬 영상 좌표 추출 스크립트 작성 및 데이터 추가
- LSTM 모델 학습: 25개 원본 → 1025개 증강 → 69.76% 정확도
- **결론**: 데이터 부족으로 70% 벽 돌파 어려움. 각 단어당 최소 15-20개 다양한 영상 필요

---

## 간단한 LSTM 모델 학습
> LSTM 모델 코드 URL `https://github.com/Kanggwanju/python-project202508/blob/hand/LSTM_model/simple_sign_language_model_no_flip.py`

## 영상 촬영
- 직접 수학, 일요일, 미국, 무한, 월세에 대한 영상 4개를 촬영

## 좌표 추출
- 코드: `https://github.com/Kanggwanju/python-project202508/blob/hand/extractor/local_video_extractor.py`
- 촬영한 영상을 이용하여 npy 파일 생성, csv 파일 수정
- 임의로 id를 501번부터 지정하여 좌표를 수정함
- 좌표 추출 명령어: `python .\extractor\local_video_extractor.py --video ".\data\monthlyRent\*.mp4" --label "월세" --id 517 --auto-increment`

---

### 수정된 csv 파일 (`https://github.com/Kanggwanju/python-project202508/tree/hand/coordinates_output`)
```text
id,label,shape,total_frames,original_fps,actual_fps,frame_skip,extracted_frames
192,수학,"(149, 57, 2)",298,59.94005994005994,29.970029970029973,2,149
240,일요일,"(99, 57, 2)",197,59.94005994005994,30.122162101857533,2,99
321,미국,"(116, 57, 2)",231,59.94005994005994,30.09977035951061,2,116
358,무한,"(121, 57, 2)",241,59.94005994005994,30.09438694085997,2,121
360,월세,"(111, 57, 2)",222,59.94005994005994,29.97002997002997,2,111
501,수학,"(56, 57, 2)",111,30.008110300081103,15.139226818058937,2,56
502,수학,"(56, 57, 2)",111,30.008110300081103,15.139226818058937,2,56
503,수학,"(49, 57, 2)",97,29.993815708101423,15.151515151515152,2,49
504,수학,"(39, 57, 2)",78,30.011542901115813,15.005771450557903,2,39
505,일요일,"(56, 57, 2)",111,30.008110300081103,15.139226818058937,2,56
506,일요일,"(53, 57, 2)",106,29.994340690435767,14.997170345217882,2,53
507,일요일,"(48, 57, 2)",96,30.00937793060332,15.004688965301655,2,48
508,일요일,"(51, 57, 2)",102,30.00882612533098,15.00441306266549,2,51
509,미국,"(67, 57, 2)",133,29.99548940009021,15.110509697789803,2,67
510,미국,"(66, 57, 2)",132,30.00681973175722,15.00340986587861,2,66
511,미국,"(62, 57, 2)",123,30.00731885825812,15.125640400097584,2,62
512,미국,"(77, 57, 2)",153,30.00588350656992,15.101000196116884,2,77
513,무한,"(62, 57, 2)",123,30.00731885825812,15.125640400097584,2,62
514,무한,"(70, 57, 2)",140,30.00428632661809,15.002143163309048,2,70
515,무한,"(77, 57, 2)",153,30.00588350656992,15.101000196116884,2,77
516,무한,"(67, 57, 2)",133,29.99548940009021,15.110509697789803,2,67
517,월세,"(71, 57, 2)",141,30.006384337093,15.10959778676314,2,71
518,월세,"(67, 57, 2)",134,30.00447828034035,15.002239140170174,2,67
519,월세,"(69, 57, 2)",137,30.00438020148927,15.111695137976348,2,69
520,월세,"(73, 57, 2)",146,30.004110152075626,15.002055076037811,2,73
```

---

## 학습 명령어 & 로그
```text
 python-project202508  python .\LSTM_model\simple_sign_language_model_no_flip.py                                                                                                            
2025-10-03 22:02:25.578524: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
총 25개의 원본 시퀀스 로드됨. 클래스 수: 5
클래스 매핑: {'수학': 0, '일요일': 1, '미국': 2, '무한': 3, '월세': 4}
증강 후 총 1025개의 시퀀스 생성됨.
패딩 후 시퀀스 shape: (1025, 204, 114)
라벨 shape: (1025, 5)
훈련 데이터 shape: (820, 204, 114), 라벨 shape: (820, 5)
테스트 데이터 shape: (205, 204, 114), 라벨 shape: (205, 5)
2025-10-03 22:02:30.268111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.    
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
C:\Users\user\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\layers\rnn\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ lstm (LSTM)                          │ (None, 204, 64)             │          45,824 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (Dropout)                    │ (None, 204, 64)             │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_1 (LSTM)                        │ (None, 32)                  │          12,416 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_1 (Dropout)                  │ (None, 32)                  │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (Dense)                        │ (None, 5)                   │             165 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 58,405 (228.14 KB)
 Trainable params: 58,405 (228.14 KB)
 Non-trainable params: 0 (0.00 B)

모델 학습 시작...
Epoch 1/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 4s 93ms/step - accuracy: 0.1753 - loss: 1.6111 - val_accuracy: 0.2195 - val_loss: 1.6089
Epoch 2/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.1860 - loss: 1.6093 - val_accuracy: 0.1646 - val_loss: 1.6083
Epoch 3/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.2287 - loss: 1.6064 - val_accuracy: 0.2195 - val_loss: 1.5826
Epoch 4/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.2180 - loss: 1.5957 - val_accuracy: 0.2744 - val_loss: 1.5649
Epoch 5/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.2424 - loss: 1.5783 - val_accuracy: 0.2805 - val_loss: 1.5144
Epoch 6/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.2470 - loss: 1.5569 - val_accuracy: 0.3659 - val_loss: 1.4781
Epoch 7/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.2637 - loss: 1.5620 - val_accuracy: 0.3293 - val_loss: 1.4862
Epoch 8/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.3399 - loss: 1.4828 - val_accuracy: 0.4756 - val_loss: 1.2935
Epoch 9/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.3811 - loss: 1.3940 - val_accuracy: 0.4268 - val_loss: 1.1856
Epoch 10/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.4421 - loss: 1.1905 - val_accuracy: 0.4268 - val_loss: 1.0792
Epoch 11/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.4345 - loss: 1.1091 - val_accuracy: 0.4329 - val_loss: 0.9989
Epoch 12/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.4024 - loss: 1.1056 - val_accuracy: 0.4390 - val_loss: 0.9928
Epoch 13/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - accuracy: 0.4497 - loss: 1.0682 - val_accuracy: 0.5244 - val_loss: 0.9091
Epoch 14/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.4162 - loss: 1.2462 - val_accuracy: 0.3049 - val_loss: 1.7017
Epoch 15/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.3277 - loss: 1.5733 - val_accuracy: 0.3902 - val_loss: 1.4694
Epoch 16/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.4299 - loss: 1.3317 - val_accuracy: 0.4268 - val_loss: 1.1385
Epoch 17/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.4284 - loss: 1.0805 - val_accuracy: 0.5183 - val_loss: 0.9533
Epoch 18/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.5412 - loss: 0.9474 - val_accuracy: 0.5915 - val_loss: 0.8008
Epoch 19/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.5534 - loss: 1.0645 - val_accuracy: 0.4573 - val_loss: 1.3320
Epoch 20/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - accuracy: 0.4588 - loss: 1.1992 - val_accuracy: 0.4146 - val_loss: 1.1624
Epoch 21/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.5076 - loss: 0.9947 - val_accuracy: 0.5122 - val_loss: 0.9362
Epoch 22/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.5747 - loss: 0.8387 - val_accuracy: 0.6037 - val_loss: 0.7751
Epoch 23/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6052 - loss: 0.7435 - val_accuracy: 0.6280 - val_loss: 0.7014
Epoch 24/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.5991 - loss: 0.7283 - val_accuracy: 0.6280 - val_loss: 0.6555
Epoch 25/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.5595 - loss: 0.8040 - val_accuracy: 0.5549 - val_loss: 0.8185
Epoch 26/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - accuracy: 0.4573 - loss: 1.3570 - val_accuracy: 0.5183 - val_loss: 0.9949
Epoch 27/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.5625 - loss: 0.8590 - val_accuracy: 0.6220 - val_loss: 0.7437
Epoch 28/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6021 - loss: 0.7250 - val_accuracy: 0.6402 - val_loss: 0.6891
Epoch 29/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6082 - loss: 0.6887 - val_accuracy: 0.6463 - val_loss: 0.6738
Epoch 30/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6479 - loss: 0.6844 - val_accuracy: 0.6159 - val_loss: 0.7322
Epoch 31/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6494 - loss: 0.6818 - val_accuracy: 0.6280 - val_loss: 0.6609
Epoch 32/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.6174 - loss: 0.7212 - val_accuracy: 0.6341 - val_loss: 0.6525
Epoch 33/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6311 - loss: 0.6775 - val_accuracy: 0.6341 - val_loss: 0.6548
Epoch 34/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6540 - loss: 0.6484 - val_accuracy: 0.6524 - val_loss: 0.6511
Epoch 35/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6585 - loss: 0.6280 - val_accuracy: 0.6585 - val_loss: 0.6296
Epoch 36/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6662 - loss: 0.6244 - val_accuracy: 0.6585 - val_loss: 0.6307
Epoch 37/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6860 - loss: 0.6114 - val_accuracy: 0.6585 - val_loss: 0.6233
Epoch 38/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6631 - loss: 0.6317 - val_accuracy: 0.6341 - val_loss: 0.6998
Epoch 39/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.6616 - loss: 0.6499 - val_accuracy: 0.6280 - val_loss: 0.6386
Epoch 40/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6585 - loss: 0.6318 - val_accuracy: 0.6524 - val_loss: 0.6023
Epoch 41/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6662 - loss: 0.6211 - val_accuracy: 0.6463 - val_loss: 0.6227
Epoch 42/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6845 - loss: 0.6269 - val_accuracy: 0.6585 - val_loss: 0.6236
Epoch 43/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6341 - loss: 0.6735 - val_accuracy: 0.6341 - val_loss: 0.6338
Epoch 44/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6265 - loss: 0.6991 - val_accuracy: 0.5732 - val_loss: 0.7400
Epoch 45/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.5899 - loss: 0.6928 - val_accuracy: 0.6037 - val_loss: 0.6446
Epoch 46/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.5930 - loss: 0.6719 - val_accuracy: 0.6463 - val_loss: 0.6202
Epoch 47/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6494 - loss: 0.6382 - val_accuracy: 0.6463 - val_loss: 0.6361
Epoch 48/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6631 - loss: 0.6018 - val_accuracy: 0.6524 - val_loss: 0.6034
Epoch 49/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6585 - loss: 0.6094 - val_accuracy: 0.6585 - val_loss: 0.5762
Epoch 50/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6662 - loss: 0.5899 - val_accuracy: 0.6585 - val_loss: 0.5746
Epoch 51/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6829 - loss: 0.5807 - val_accuracy: 0.6585 - val_loss: 0.5739
Epoch 52/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6601 - loss: 0.5839 - val_accuracy: 0.6585 - val_loss: 0.5677
Epoch 53/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6677 - loss: 0.5862 - val_accuracy: 0.6585 - val_loss: 0.5681
Epoch 54/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6677 - loss: 0.5889 - val_accuracy: 0.6585 - val_loss: 0.5660
Epoch 55/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6784 - loss: 0.5794 - val_accuracy: 0.6585 - val_loss: 0.5629
Epoch 56/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6753 - loss: 0.5839 - val_accuracy: 0.6585 - val_loss: 0.5654
Epoch 57/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6753 - loss: 0.5881 - val_accuracy: 0.6341 - val_loss: 0.5861
Epoch 58/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6890 - loss: 0.5925 - val_accuracy: 0.6463 - val_loss: 0.5728
Epoch 59/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6707 - loss: 0.5844 - val_accuracy: 0.6463 - val_loss: 0.5685
Epoch 60/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6494 - loss: 0.5911 - val_accuracy: 0.6585 - val_loss: 0.5650
Epoch 61/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6540 - loss: 0.5811 - val_accuracy: 0.6585 - val_loss: 0.5655
Epoch 62/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6997 - loss: 0.5764 - val_accuracy: 0.6585 - val_loss: 0.5648
Epoch 63/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6936 - loss: 0.5747 - val_accuracy: 0.6585 - val_loss: 0.5646
Epoch 64/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6677 - loss: 0.5828 - val_accuracy: 0.6707 - val_loss: 0.5641
Epoch 65/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - accuracy: 0.7043 - loss: 0.5646 - val_accuracy: 0.6768 - val_loss: 0.5571
Epoch 66/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6738 - loss: 0.6037 - val_accuracy: 0.6402 - val_loss: 0.6544
Epoch 67/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6494 - loss: 0.6097 - val_accuracy: 0.6341 - val_loss: 0.5702
Epoch 68/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - accuracy: 0.6662 - loss: 0.5947 - val_accuracy: 0.6463 - val_loss: 0.5715
Epoch 69/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6692 - loss: 0.5955 - val_accuracy: 0.6524 - val_loss: 0.5967
Epoch 70/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6585 - loss: 0.5851 - val_accuracy: 0.6159 - val_loss: 0.7898
Epoch 71/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - accuracy: 0.6707 - loss: 0.5880 - val_accuracy: 0.6585 - val_loss: 0.5661
Epoch 72/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6829 - loss: 0.5778 - val_accuracy: 0.6646 - val_loss: 0.5637
Epoch 73/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6921 - loss: 0.5755 - val_accuracy: 0.6585 - val_loss: 0.5677
Epoch 74/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6905 - loss: 0.5765 - val_accuracy: 0.6585 - val_loss: 0.5759
Epoch 75/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6814 - loss: 0.5952 - val_accuracy: 0.6524 - val_loss: 0.6785
Epoch 76/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.7165 - loss: 0.5728 - val_accuracy: 0.6829 - val_loss: 0.5599
Epoch 77/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.7088 - loss: 0.5569 - val_accuracy: 0.6646 - val_loss: 0.6350
Epoch 78/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
Epoch 79/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6555 - loss: 0.6541 - val_accuracy: 0.6280 - val_loss: 0.6692
Epoch 80/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
Epoch 79/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6555 - loss: 0.6541 - val_accuracy: 0.6280 - val_loss: 0.6692
Epoch 80/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6799 - loss: 0.5958 - val_accuracy: 0.6280 - val_loss: 0.6918
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
Epoch 79/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6555 - loss: 0.6541 - val_accuracy: 0.6280 - val_loss: 0.6692
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
Epoch 79/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - accuracy: 0.6738 - loss: 0.6247 - val_accuracy: 0.5793 - val_loss: 0.9499
Epoch 79/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6555 - loss: 0.6541 - val_accuracy: 0.6280 - val_loss: 0.6692
Epoch 80/200
21/21 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - accuracy: 0.6799 - loss: 0.5958 - val_accuracy: 0.6280 - val_loss: 0.6918

모델 평가 시작...
테스트 손실: 0.5491
테스트 정확도: 0.6976

예측 예시:
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 181ms/step
실제 라벨: 미국
예측 라벨: 미국
예측 확률: [4.9356907e-04 1.8455979e-04 9.8956901e-01 9.5121451e-03 2.4076115e-04]
```

---

## 예측 확률 분석

```python
예측 확률: [4.9356907e-04 1.8455979e-04 9.8956901e-01 9.5121451e-03 2.4076115e-04]
```

이건 **과학적 표기법(scientific notation)**으로 표시된 거예요:

**실제 값으로 변환하면:**
- `4.9356907e-04` = 0.000494 (0.049%)
- `1.8455979e-04` = 0.000185 (0.019%)
- `9.8956901e-01` = **0.989569 (98.96%)** ← 미국
- `9.5121451e-03` = 0.009512 (0.95%)
- `2.4076115e-04` = 0.000241 (0.024%)

**해석:**
- 모델이 3번째 클래스(미국)을 **98.96% 확률**로 예측
- 나머지 클래스들은 모두 1% 미만
- 실제 라벨도 "미국"이므로 **정확하게 맞춤**

## 왜 이렇게 표시될까?

확률이 0.001 이하처럼 매우 작을 때 Python/NumPy는 자동으로 과학적 표기법을 사용합니다:
- `e-04` = 10^-4 = 0.0001
- `e-01` = 10^-1 = 0.1

이건 매우 좋은 신호입니다. 모델이 확신을 가지고(98.96%) 정답을 맞추고 있다는 뜻이거든요!

---

## 클로드의 현재 상황 분석

**긍정적인 부분:**
- 모델이 학습은 되고 있음 (초반 17% → 최종 69.76% 정확도)
- 과적합은 심하지 않음 (train/val 정확도 차이가 크지 않음)
- 데이터 증강이 작동하고 있음 (25개 → 1025개)

**문제점:**
1. **정확도가 70% 근처에서 정체됨**
    - Epoch 35 이후로 거의 개선이 없음 (65.85% 근처에서 반복)
    - 5개 클래스 분류에서 70%는 낮은 편

2. **데이터가 여전히 부족함**
    - 5개 단어 × 5개 영상 = 25개 원본 데이터
    - 증강으로 1025개 만들었지만, 원본 다양성이 부족
    - 같은 사람이 찍은 같은 패턴만 학습

3. **학습이 불안정함**
    - val_loss가 튀는 구간들 (Epoch 14, 19, 44, 70, 78 등)
    - 모델이 일반화를 어려워하는 신호

## 추천 개선 방안

**1. 더 많은 실제 영상 필요**
```
각 단어당 최소 10-20개 영상 권장
- 다른 사람들이 찍은 영상
- 다른 각도, 다른 속도로 찍은 영상
- 조명이 다른 환경
```

**2. 증강 횟수 조정**
현재 `num_augmentations=40`은 과도할 수 있습니다.
원본이 5개밖에 없는데 각각을 40번씩 증강하면 거의 같은 데이터를 반복 학습하는 것과 비슷합니다.

**3. 모델 복잡도 줄이기**
데이터가 적을 때는 더 단순한 모델이 나을 수 있습니다.

**현실적인 판단:**
5개 단어에 각 5개 영상으로 70% 정확도면 나쁘지 않은 출발점이지만,
실용적으로 사용하기엔 부족합니다.
각 단어당 최소 15-20개의 **서로 다른** 영상을 확보하는 게 우선입니다.


